{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "import mlstream_spark_udfs as udfs\n",
    "\n",
    "# Create a spark session and add the jar file with mlstream-spark-udfs\n",
    "spark = SparkSession.builder \\\n",
    "    .appName('demo approx_topk') \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\",\"4G\") \\\n",
    "    .config(\"spark.executor.memory\",\"4G\")\\\n",
    "    .config(\"spark.jars\", os.environ[\"MLSTREAM_UDFS_JAR_PATH\"]) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Note: Spark UI should be available at http://localhost:4040\n",
    "udfs.registerUDFs(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "|  5|\n",
      "|  6|\n",
      "|  7|\n",
      "|  8|\n",
      "|  9|\n",
      "+---+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create two sequence of integers (0, 1, ..., n - 1] and (0, 1, 2,...,4],\n",
    "# then take the union. It's clear that the duplicate elements are (0, 1, ...,4]\n",
    "n = 200000000\n",
    "# If you increase n even more, spark is likely to crash with \"OutOfMemory\" exception\n",
    "spark.range(0, n,  step = 1, numPartitions = 4).createOrReplaceTempView(\"lots_of_data\")\n",
    "spark.range(0, 5,  step = 1, numPartitions = 4).createOrReplaceTempView(\"duplicates\")\n",
    "spark.sql(\"\"\"(\n",
    "(SELECT * FROM lots_of_data) UNION ALL \n",
    "(SELECT * FROM duplicates)\n",
    ")\"\"\").createOrReplaceTempView(\"test_table\")\n",
    "spark.sql(\"\"\"SELECT * FROM test_table\"\"\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+------+\n",
      "|      key|value| error|\n",
      "+---------+-----+------+\n",
      "|        0|  1.0|1525.0|\n",
      "|        1|  1.0|1525.0|\n",
      "|        2|  1.0|1525.0|\n",
      "|        3|  1.0|1525.0|\n",
      "|        4|  1.0|1525.0|\n",
      "|149946368|  1.0|1525.0|\n",
      "|149946369|  1.0|1525.0|\n",
      "|149946370|  1.0|1525.0|\n",
      "|149946371|  1.0|1525.0|\n",
      "|149946372|  1.0|1525.0|\n",
      "+---------+-----+------+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 4 ms, sys: 0 ns, total: 4 ms\n",
      "Wall time: 8.27 s\n"
     ]
    }
   ],
   "source": [
    "def query_1():\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT extract_approx_topk(d) FROM\n",
    "            (SELECT \n",
    "                approx_topk(CAST (id AS LONG), CAST(1.0 AS FLOAT), 1000, \"1MB\") AS d \n",
    "             FROM test_table) tmp\n",
    "    \"\"\").show(10)\n",
    "%time query_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---+\n",
      "|  id|cnt|\n",
      "+----+---+\n",
      "|   0|  2|\n",
      "|   1|  2|\n",
      "|   3|  2|\n",
      "|   2|  2|\n",
      "|   4|  2|\n",
      "|  26|  1|\n",
      "|  29|  1|\n",
      "| 474|  1|\n",
      "| 964|  1|\n",
      "|1677|  1|\n",
      "+----+---+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 8 ms, sys: 4 ms, total: 12 ms\n",
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "# You can note in the spark logs (or the docker console) the following messsage\n",
    "# 19/11/19 17:44:10 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
    "# Older versions of spark would crash under this circumstance\n",
    "def query_2():\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT id, COUNT(*) AS cnt\n",
    "        FROM test_table\n",
    "        GROUP BY id\n",
    "        ORDER BY cnt DESC\n",
    "        LIMIT 1000\n",
    "    \"\"\").show(10)\n",
    "%time query_2() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+\n",
      "|       id|cnt|\n",
      "+---------+---+\n",
      "|        0|  2|\n",
      "|        1|  2|\n",
      "|        3|  2|\n",
      "|        2|  2|\n",
      "|        4|  2|\n",
      "|199229684|  1|\n",
      "|199230068|  1|\n",
      "|199230140|  1|\n",
      "|199230304|  1|\n",
      "|199230375|  1|\n",
      "+---------+---+\n",
      "only showing top 10 rows\n",
      "\n",
      "CPU times: user 0 ns, sys: 4 ms, total: 4 ms\n",
      "Wall time: 37.9 s\n"
     ]
    }
   ],
   "source": [
    "# Exact calculation where the filter comes from approx_topk\n",
    "def query_3():\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT extract_approx_topk(d) FROM\n",
    "            (SELECT \n",
    "                approx_topk(CAST (id AS LONG), CAST(1.0 AS FLOAT), 1000, \"10MB\") AS d \n",
    "             FROM test_table) tmp\n",
    "    \"\"\").createOrReplaceTempView(\"candidate_ids\")\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "            SELECT id, COUNT(id) AS cnt FROM candidate_ids c\n",
    "            LEFT JOIN (SELECT * FROM test_table) t ON c.key = t.id\n",
    "            GROUP BY id\n",
    "            ORDER BY cnt DESC\n",
    "            LIMIT 1000            \n",
    "    \"\"\").show(10)    \n",
    "\n",
    "%time query_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute Candiates\n",
      "+---------+-----+-----+\n",
      "|      key|value|error|\n",
      "+---------+-----+-----+\n",
      "|        0|  1.0| 95.0|\n",
      "|        1|  1.0| 95.0|\n",
      "|        2|  1.0| 95.0|\n",
      "|        3|  1.0| 95.0|\n",
      "|        4|  1.0| 95.0|\n",
      "|199229440|  1.0| 95.0|\n",
      "|199229441|  1.0| 95.0|\n",
      "|199229442|  1.0| 95.0|\n",
      "|199229443|  1.0| 95.0|\n",
      "|199229444|  1.0| 95.0|\n",
      "+---------+-----+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "Compute Exact Values For Candiates\n",
      "+---+---+\n",
      "| id|cnt|\n",
      "+---+---+\n",
      "|  1|  2|\n",
      "|  3|  2|\n",
      "|  2|  2|\n",
      "|  4|  2|\n",
      "+---+---+\n",
      "\n",
      "CPU times: user 8 ms, sys: 0 ns, total: 8 ms\n",
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "# Exact calculation where the filter comes from approx_topk\n",
    "def query_4():\n",
    "    print(\"Compute Candiates\")\n",
    "    spark.sql(\"\"\"\n",
    "        SELECT extract_approx_topk(d) FROM\n",
    "            (SELECT \n",
    "                approx_topk(CAST (id AS LONG), CAST(1.0 AS FLOAT), 1000, \"10MB\") AS d \n",
    "             FROM test_table) tmp\n",
    "    \"\"\").show(10)\n",
    "    \n",
    "    print(\"Compute Exact Values For Candidates\")\n",
    "    spark.sql(\"\"\"\n",
    "            SELECT id, COUNT(id) AS cnt \n",
    "            FROM test_table t\n",
    "            WHERE id IN (1,2,3,4) -- TODO: add function approx_topk_is_frequent(d) so no hard-coding is needed\n",
    "            GROUP BY id\n",
    "            ORDER BY cnt DESC\n",
    "            LIMIT 1000            \n",
    "    \"\"\").show(10)    \n",
    "\n",
    "%time query_4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# - improve errors to report not the \"global\" errors, but errors for each element\n",
    "# - rename approx_topk -> approx_most_frequent"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
